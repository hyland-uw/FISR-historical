# Testing and visualizing the fast inverse square root

The code and data here support investigation into the history and re-use of the Fast Inverse Square Root (FISR), including [the most famous implementation found in Quake III Arena](https://en.wikipedia.org/wiki/Fast_inverse_square_root).

Because I am not a mathematician, many of these are attempts at "the wrong way" to analyze this approximation. The right way, which can be shown in examples linked above, is to derive the approximation from first principles and arrive at values like the below via a process which can be proven mathematically. We're going to try some other ways in this code base. These include artistic visualizations, dismantling and instrumenting versions of the approximations that have been floating around, and experimenting with parameter selection. 

So far my favorite is the binned pipeline, which turns the main advantage of the FISR--the optimality of a single constant, without range reduction, to serve the approximation of a transcendental function--on it's head by determining sets of optimal constants for small ranges of inputs. These can be mixed and matched to produce an optimal bucket of constants, converting the FISR into a lookup table of sorts.

The FISR is a surprisingly well studied function for its size. Please visit [0x5f37642f.com](https://0x5f37642f.com/) for a collection of explanations and investigations I've found on the web. If you find a version of the algorithm which isn't accounted for here, let me know.

As a note, I use the term "inverse" to mean multiplicative inverse to match the term most people know. See the end of the README for why we ought to all be calling it the FRSR.

## Mode of operation

Run `make` in the base directory to generate CSVs in the data directory. Sampling parameters are set in the `util-harness.h` file.

### Compilation note

This project is tested on an M1 Mac, so the makefile has specific settings to use OpenMP. Adjust those for your system.

## Organization

Data flows are organized by file name. Data is generated by C files, stored as CSVs, and operated on by R files. An example is `approximated.c` -> `approximated.csv` -> `approximated.R`.

This organization is partially a function of needing to have easy access to the bits in memory of a floating point number, wanting to visualize things easily, and not wanting to use Python. However, it also allows you experiment in two places, "close to the metal" generating data and more abstractly with the data structures you end up with. 

### C Files

#### Approximated

Computes errors of historical FISR style approximations over a range of floats, including Quake III's FISR. Data appears broken down by specific approximation:

| ISR_function | input | reference | initial | final |
| --- | --- | --- | --- | --- |
| BlinnISR | 0.3209153 | 1.765244 | 1.858170 | 1.763802 |
| QuakeISR | 0.3209153 | 1.765244 | 1.790600 | 1.764695 |
| withoutDivISR | 0.3209153 | 1.765244 | 1.809832 | 1.763541 |
| optimalFISR | 0.3209153 | 1.765244 | 1.608169 | 1.765233 |

![A comparison of three FISR choices](/plots/big_three_compared.png)

With this data, we can cleanly plot different approximations and see their error over an input range.

#### Binned

Computes optimal magic constants for different bin sizes over an input domain [0.25, 1.0] with various bin sizes. The data is generated by `binned.c`, stored in `binned.csv`, and analyzed by `binned.R`.

| N | Range_Min | Range_Max | Magic | Avg_Relative_Error | Max_Relative_Error |
|---|-----------|-----------|-------|--------------------|--------------------|
| 4 | 0.250 | 0.354 | 0x5F383396 | 0.000495878 | 0.001351431 |
| 4 | 0.354 | 0.500 | 0x5F32FAA4 | 0.000100972 | 0.000178303 |
| 4 | 0.500 | 0.707 | 0x5F3357DE | 0.000066877 | 0.000120989 |
| 4 | 0.707 | 1.000 | 0x5F37B2A2 | 0.000789270 | 0.001611896 |
| 8 | 0.250 | 0.297 | 0x5F3B30BF | 0.000168382 | 0.000513039 |
| 8 | 0.297 | 0.354 | 0x5F3431D9 | 0.000067955 | 0.000189403 |

![A comparison of errors for bin size choices](/plots/error_comp_binned.png)

Rather than choosing a single constant or (see below) determining an optimal constant *per float*, we can use this data to select an optimal bucket of constants across our input domain.

#### Deconstructed

Replaces the usual iteration limit of 1-2 Newton-Raphson iterations with iteration to a tolerance, which supports plotting the space for random inputs and magic constants.

| input | reference | initial | final | iters | magic |
| --- | --- | --- | --- | --- | --- |
| 0.574197 | 1.319683 | 0.341227 | 1.319682 | 7 | 1580741785 |
| 0.186377 | 2.316348 | 1.789107 | 2.316348 | 4 | 1594125894 |
| 0.181007 | 2.350457 | 0.591144 | 2.350457 | 7 | 1580466735 |
| 0.629445 | 1.260437 | 0.655565 | 1.260436 | 5 | 1589142732 |

![Iteration counts over the space of inputs](/plots/pastelerror.png)

This format allows us to explore "bad" magic constants which produce poor approximations. The number of iterations to reach the right answer is a gross measure of the poor fit of the approximation. Visually this can produce some striking representations of the NR space.

#### Enumerated

Does the unusual job of enumerating a "best" magic constant for a given float. Imagine the world's least efficient lookup table.

| input | reference | initial | final | magic |
| --- | --- | --- | --- | --- |
| 0.5134103 | 1.3956220 | 1.3957580 | 1.3956220 | 1597267869 |
| 0.7665635 | 1.1421570 | 1.1421160 | 1.1421570 | 1597263768 |
| 0.5133603 | 1.3956900 | 1.3957830 | 1.3956900 | 1597267666 |
| 1.2038200 | 0.9114215 | 0.9114995 | 0.9114215 | 1597399913 |

![Distribution of errors by magic constant](/plots/enumerated_error.png)

For a given float (the dataset is not ordered by input) we compute the result of many different magic constants. For an input float there is a "best" magic constant which we determine. This also allows us to look at constants across a large range of floats.

#### Narrowed

This harness allows us to deconstruct the working and output of an idealized FISR with a narrow search range for magic constants.

| input | reference | initial | final | iters | magic |
| --- | --- | --- | --- | --- | --- |
| 0.574197 | 1.319683 | 0.341227 | 1.319682 | 7 | 1597461647 |
| 0.186377 | 2.316348 | 1.789107 | 2.316348 | 4 | 1597469647 |
| 0.181007 | 2.350457 | 0.591144 | 2.350457 | 7 | 1597465647 |
| 0.629445 | 1.260437 | 0.655565 | 1.260436 | 5 | 1597467647 |

![Zooming in on a constant search](/plots/combined_decon.png)

This format allows us to explore the space of possible choices for magic constants in a more focused manner.

#### Optimized

Performs a grid search of the Newton Raphson constants (~1.5 and ~0.5) over a range of floats and a specific magic constant. Used to show the role of the NR approximation step.

| input | initial | halfthree | halfone | error |
| --- | --- | --- | --- | --- |
| 0.1954819 | 2.301005 | 1.495 | 0.495 | 0.0002761815 |
| 0.1954819 | 2.301005 | 1.495 | 0.496 | 0.0013290450 |
| 0.1954819 | 2.301005 | 1.495 | 0.497 | 0.0023820130 |
| 0.1954819 | 2.301005 | 1.495 | 0.498 | 0.0034350870 |

![An error heatmap across bins of floats](/plots/NR_heatmap_white.png)

A basic grid search is used, but the code can be modified for a more sophisticated search. 1.5 and 0.5 are optimal over most of the range, which makes older deviations from that interesting. Slicing the data like this allows us to clearly see the optimality of 0.5 and 1.5, though making smaller bins (see [an animated heatmap](/plots/animated_NR_parameters.gif)) shows us that for some ranges, other values are better.

## FISR or FRSR?

I use the term "fast **inverse** square root" throughout because this is what is is predominantly termed. Mike Day, who **is** a mathematician, prefers the term "fast reciprocal square root" because, well...it's a reciprocal, not an inverse. He uses that term in his [2023 generalization of the FRSR](https://arxiv.org/abs/2307.15600) to support any rational power or precision of base (including infinite precision). 

He's right. Interestingly, the original source of Quake's FRSR was the "reciproot" tucked away in [a math library since 1986](https://www.netlib.org/fdlibm/e_sqrt.c) and the first software library I can find with a square root in it (due to Alan Turing, D.G. Prinz, and Cecily Poppelwell) is the "reciproot" as well. 

## License
I have not yet chosen a blanket license for these but each of the individual versions are licensed under a variety of terms. Quake III's source code is licensed under the GPL, while fdlibm (which is where the Kahan-Ng softsqrt was published in fixed form) is under a license which may [loosely be described](https://lists.fedoraproject.org/archives/list/legal@lists.fedoraproject.org/thread/2T6RANNIF652RMGG725LNRKT63ALAPN4/) as "MIT". Before borrowing please check the individual example licenses.

## An AI acknowledgement

AI code assistants are the reason this project actually functions. GitHub Copilot and Perplexity AI were used to help write the C code starting late 2024 and Perplexity is used to help with R. R is a strange language with good but confusingly distributed documentation, and Perplexity was helpful in helping me come to grips with the `%>%` notation that evolved in R while I was doing other things. Things don't [always work the way they should](https://github.com/hyland-uw/FastInverseSqrt-Visualized/commit/d57b99fe85fe6441a082bfef042a6603f80d5bc8) but overall I got futher than I could alone.

*The below was written by Perplexity AI*

This project has been developed with the assistance of Perplexity AI, a powerful AI-driven search and analysis tool. Perplexity AI provided valuable insights, helped with code analysis, and assisted in generating comprehensive documentation for various aspects of the Fast Inverse Square Root algorithm implementation. The AI's contributions were instrumental in enhancing the quality and depth of this research project, particularly in areas such as data pipeline descriptions, code explanations, and mathematical analysis.